
@article{bezanson_julia_2017,
  title = {Julia: A {{Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  date = {2017-01-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/abs/10.1137/141000671},
  urldate = {2021-03-02},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: \textbackslash beginlist \textbackslash item  High-level dynamic programs have to be slow. \textbackslash item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  file = {/Users/mwbc/Zotero/storage/AV4TDMMY/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/Users/mwbc/Zotero/storage/FZ5I4DGI/141000671.html}
}


@report{bierlaire_short_2020,
  title = {A Short Introduction to {{PandasBiogeme}}},
  author = {Bierlaire, Michel},
  date = {2020-06-05},
  number = {TRANSP-OR 200605},
  pages = {22},
  institution = {{Ecole Poltechnique Fédérale de Lausanne}},
  location = {{Lausanne}},
  url = {https://transp-or.epfl.ch/documents/technicalReports/Bier20.pdf},
  langid = {english},
  file = {/Users/mwbc/Zotero/storage/R3HD5SSQ/Bierlaire - A short introduction to PandasBiogeme.pdf}
}



@article{hess_apollo_2019,
  title = {Apollo: A Flexible, Powerful and Customisable Freeware Package for Choice Model Estimation and Application},
  shorttitle = {Apollo},
  author = {Hess, Stephane and Palma, David},
  date = {2019-09},
  journaltitle = {Journal of Choice Modelling},
  shortjournal = {Journal of Choice Modelling},
  volume = {32},
  pages = {100170},
  issn = {17555345},
  doi = {10.1016/j.jocm.2019.100170},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1755534519300703},
  urldate = {2021-11-21},
  abstract = {The community of choice modellers has expanded substantially over recent years, covering many disciplines and encompassing users with very different levels of econometric and computational skills. This paper presents an introduction to Apollo, a powerful new freeware package for R that aims to provide a comprehensive set of modelling tools for both new and experienced users. Apollo also incorporates numerous post-estimation tools, allows for both classical and Bayesian estimation, and permits advanced users to develop their own routines for new model structures.},
  langid = {english},
  file = {/Users/mwbc/Zotero/storage/3PZ6VB2Y/Hess and Palma - 2019 - Apollo A flexible, powerful and customisable free.pdf}
}


@article{mogensen_optim_2018,
  author  = {Mogensen, Patrick Kofod and Riseth, Asbjørn Nilsen},
  title   = {Optim: A mathematical optimization package for {Julia}},
  journal = {Journal of Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {24},
  pages   = {615},
  doi     = {10.21105/joss.00615}
}

@article{revels_forward_2016,
    title = {Forward-Mode Automatic Differentiation in {J}ulia},
   author = {{Revels}, J. and {Lubin}, M. and {Papamarkou}, T.},
  journal = {arXiv:1607.07892 [cs.MS]},
     year = {2016},
      url = {https://arxiv.org/abs/1607.07892}
}


@dataset{ruggles_ipums_2019,
  title = {{{IPUMS USA}}: Version 9.0},
  shorttitle = {{{IPUMS USA}}},
  author = {Ruggles, Steven and Flood, Sarah and Goeken, Ronald and Grover, Josiah and Meyer, Erin and Pacas, Jose and Sobek, Matthew},
  date = {2019},
  publisher = {{Minneapolis, MN: IPUMS}},
  doi = {10.18128/D010.V9.0},
  url = {https://www.ipums.org/projects/ipums-usa/d010.V9.0},
  urldate = {2020-03-14},
  abstract = {IPUMS USA contains harmonized census and American Community Survey (ACS) data from 1790 to the present. For the period 1850 to 1940, IPUMS includes 100\% of individuals in the decennial censuses. For the period 1950 to 2010, we have public use samples of decennial censuses. For the period since 2000, IPUMS provides annual ACS public use samples. The available information in the censuses and ACS varies by year, but generally includes basic housing data (type of housing, household appliances, and housing characteristics), demographic data (age, race, sex, marital status, and relationship to the head of household), economic data (occupation, industry, income, work status), and other individual characteristics (migration, disability, veteran status). For the period 1790 to 1840, IPUMS includes household level records for 100\% of households. IPUMS USA is available through an online data access system free of charge to registered users at usa.ipums.org.},
  editora = {United States Census Bureau},
  editoratype = {collaborator},
  langid = {english}
}


@inproceedings{shi_distributed_2019,
  title = {Distributed {{Logistic Regression}} for {{Separated Massive Data}}},
  booktitle = {Big {{Data}}},
  author = {Shi, Peishen and Wang, Puyu and Zhang, Hai},
  editor = {Jin, Hai and Lin, Xuemin and Cheng, Xueqi and Shi, Xuanhua and Xiao, Nong and Huang, Yihua},
  date = {2019},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {285--296},
  publisher = {{Springer}},
  location = {{Singapore}},
  doi = {10.1007/978-981-15-1899-7_20},
  abstract = {In this paper, we study the distributed logistic regression to process the separated large scale data which is stored in different linked computers. Based on the Alternating Direction Method of Multipliers (ADMM) algorithm, we transform the solving of logistic problem into the multistep iteration process, and propose the distributed logistic algorithm which has controllable communication cost. Specifically, in each iteration of the distributed algorithm, each computer updates the local estimators and interacts the local estimators with the neighbors simultaneously. Then we prove the convergence of distributed logistic algorithm. Due to the decentralized property of computer network, the proposed distributed logistic algorithm is robust. The classification results of our distributed logistic method are same as the non-distributed approach. Numerical studies have shown that our approach are both effective and efficient which perform well in distributed massive data analysis.},
  isbn = {9789811518997},
  langid = {english},
  keywords = {ADMM algorithm,Distributed,Logistic regression},
  file = {/Users/mwbc/Zotero/storage/TMMPEB6C/Shi et al. - 2019 - Distributed Logistic Regression for Separated Mass.pdf}
}


@article{gopal_distributed_2013,
  title = {Distributed Training of {{Large}}-Scale {{Logistic}} Models},
  author = {Gopal, Siddharth and Yang, Yiming},
  date = {2013},
  pages = {9},
  abstract = {Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands) and the dimensionality is high. In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time.},
  langid = {english},
  file = {/Users/mwbc/Zotero/storage/BWDAARX9/Gopal and Yang - Distributed training of Large-scale Logistic model.pdf}
}



@article{zwaenepoel_inference_2019,
  title = {Inference of {{Ancient Whole-Genome Duplications}} and the {{Evolution}} of {{Gene Duplication}} and {{Loss Rates}}},
  author = {Zwaenepoel, Arthur and Van de Peer, Yves},
  editor = {Wright, Stephen},
  date = {2019-07-01},
  journaltitle = {Molecular Biology and Evolution},
  volume = {36},
  number = {7},
  pages = {1384--1404},
  issn = {0737-4038, 1537-1719},
  doi = {10.1093/molbev/msz088},
  url = {https://academic.oup.com/mbe/article/36/7/1384/5475503},
  urldate = {2021-12-13},
  abstract = {Gene tree–species tree reconciliation methods have been employed for studying ancient whole-genome duplication (WGD) events across the eukaryotic tree of life. Most approaches have relied on using maximum likelihood trees and the maximum parsimony reconciliation thereof to count duplication events on specific branches of interest in a reference species tree. Such approaches do not account for uncertainty in the gene tree and reconciliation, or do so only heuristically. The effects of these simplifications on the inference of ancient WGDs are unclear. In particular, the effects of variation in gene duplication and loss rates across the species tree have not been considered. Here, we developed a full probabilistic approach for phylogenomic reconciliation-based WGD inference, accounting for both gene tree and reconciliation uncertainty using a method based on the principle of amalgamated likelihood estimation. The model and methods are implemented in a maximum likelihood and Bayesian setting and account for variation of duplication and loss rates across the species tree, using methods inspired by phylogenetic divergence time estimation. We applied our newly developed framework to ancient WGDs in land plants and investigated the effects of duplication and loss rate variation on reconciliation and gene count based assessment of these earlier proposed WGDs.},
  langid = {english},
  file = {/Users/mwbc/Zotero/storage/P7LGQID4/Zwaenepoel and Van de Peer - 2019 - Inference of Ancient Whole-Genome Duplications and.pdf}
}



